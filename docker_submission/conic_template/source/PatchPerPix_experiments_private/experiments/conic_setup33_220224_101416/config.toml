base = "experimentsTmp/conic_setup33_220224_101416"

[general]
logging = 20
debug = false
overwrite = false

[data]
train_data = "/home/peter/data/datasets/data_conic/lizard_zarr/train"
val_data = "/home/peter/data/datasets/data_conic/lizard_zarr/val/patches"
test_data = "images.npy"
voxel_size = [ 1, 1,]
input_format = "zarr"
gt_key = "volumes/gt_instances"
raw_key = "volumes/raw"
num_channels = 3
validate_on_train = false

[model]
train_net_name = "train_net"
test_net_name = "test_net"
train_input_shape = [ 256, 256,]
test_input_shape = [ 256, 256,]
patchshape = [ 1, 25, 25,]
patchstride = [ 1, 1, 1,]
num_fmaps = 40
fmap_inc_factors = [ 2, 2, 2, 2,]
fmap_dec_factors = [ 1.3, 1.3, 1.3, 1.3,]
downsample_factors = [ [ 2, 2,], [ 2, 2,], [ 2, 2,], [ 2, 2,],]
activation = "relu"
padding = "same"
kernel_size = 3
num_repetitions = 2
upsampling = "resize_conv"
code_units = 256
autoencoder_chkpt = "this"
focal_class_loss = true

[autoencoder]
train_net_name = "train_net"
test_net_name = "test_net"
train_input_shape = [ 1, 25, 25,]
test_input_shape = [ 1, 25, 25,]
patchshape = [ 1, 25, 25,]
patchstride = [ 1, 1, 1,]
network_type = "conv"
activation = "relu"
num_fmaps = [ 32, 64, 128,]
downsample_factors = [ [ 2, 2,], [ 2, 2,], [ 2, 2,],]
upsampling = "resize_conv"
kernel_size = 3
num_repetitions = 2
padding = "same"
code_method = "conv1x1_b"
code_units = 256
code_fmaps = 16
code_activation = "sigmoid"
regularizer = "l2"
regularizer_weight = 0.0001
overlapping_inst = false

[optimizer]
loss = "ce"
optimizer = "Adam"
lr = 0.0001

[preprocessing]
clipmax = 1500

[training]
batch_size = 4
num_gpus = 1
num_workers = 24
cache_size = 60
max_iterations = 100000
checkpoints = 1000
snapshots = 500
profiling = 100
train_code = true

[prediction]
output_format = "hdf"
aff_key = "volumes/pred_affs"
code_key = "volumes/pred_code"
fg_key = "volumes/pred_fgbg"
fg_thresh = 0.5
decode_batch_size = 1024
test_time_aug = false

[validation]
params_product = [ "mws", "fg_thresh_vi",]
params_zip = [ "patch_threshold", "fc_threshold",]
mws = [ true,]
fg_thresh_vi = [ -1.0,]
patch_threshold = [ 0.4, 0.5, 0.6, 0.7,]
fc_threshold = [ 0.4, 0.5, 0.6, 0.7,]
checkpoints = [ 50000,]

[vote_instances]
patch_threshold = 0.6
fc_threshold = 0.6
cuda = true
blockwise = false
num_parallel_samples = 1
num_parallel_blocks = 1
num_workers = 8
debug = false
select_patches_for_sparse_data = true
save_no_intermediates = true
output_format = "hdf"
parallel = false
includeSinglePatchCCS = true
sample = 1.0
removeIntersection = true
mws = true
isbiHack = false
mask_fg_border = false
graphToInst = false
skipLookup = false
skipConsensus = false
skipRanking = false
skipThinCover = false
affinity_graph_voting = false
affinity_graph_voting_selected = false
termAfterThinCover = false
pad_with_ps = true
fg_thresh_vi = -1.0
consensus_interleaved_cnt = false
consensus_norm_prob_product = true
consensus_prob_product = true
consensus_norm_aff = true
vi_bg_use_inv_th = true
vi_bg_use_half_th = false
vi_bg_use_less_than_th = false
rank_norm_patch_score = true
rank_int_counter = false
patch_graph_norm_aff = true
blockwise_old_stitch_fn = false
only_bb = false
overlap = [ 0, 0, 5,]
flip_cons_arr_axes = false
return_intermediates = false
use_score_oracle = false
store_scores = false
shuffle_patches = false

[evaluation]
num_workers = 1
res_key = "vote_instances"
filterSz = 0
metric = "confusion_matrix.th_0_5.avAP"
print_f_factor_perc_gt_0_8 = false
use_linear_sum_assignment = true
foreground_only = false
summary = [ "confusion_matrix.avAP", "confusion_matrix.th_0_1.AP", "confusion_matrix.th_0_2.AP", "confusion_matrix.th_0_3.AP", "confusion_matrix.th_0_4.AP", "confusion_matrix.th_0_5.AP", "confusion_matrix.th_0_6.AP", "confusion_matrix.th_0_7.AP", "confusion_matrix.th_0_8.AP", "confusion_matrix.th_0_9.AP",]
prediction_only = false

[postprocessing]
remove_small_comps = 600

[visualize]
save_mip = true

[evaluation.prediction]
eval_fg_prediction = false
eval_fg_thresholds = [ 0.2, 0.5, 0.8,]
eval_patch_prediction = true
eval_patch_thresholds = [ 0.6,]
return_patches = true
store_iou = true
eval_prediction_output = false
evaluate_skeleton_coverage = false
remove_small_comps = [ 0,]

[postprocessing.watershed]
output_format = "hdf"

[training.augmentation.elastic]
control_point_spacing = [ 10, 10,]
jitter_sigma = [ 1, 1,]
rotation_min = -45
rotation_max = 45

[training.augmentation.intensity]
scale = [ 0.9, 1.1,]
shift = [ -0.1, 0.1,]

[training.augmentation.simple]
